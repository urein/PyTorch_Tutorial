{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch提供了两个主要的特性：\n",
    "1. 类似于ndarray的Tensor，但它可以在GPU上进行计算\n",
    "2. 自动微分帮助我们训练NN\n",
    "\n",
    "下面的例子用一个包含四个参数的多项式来拟合y=sin(x)。损失函数使用欧式距离。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up: numpy\n",
    "\n",
    "numpy只提供基础的矩阵运算，不涉及深度学习，梯度下降。下面我们用numpy手动通过前向和反向传播实现上面的拟合任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一些输入输出数据\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机初始化参数\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 8.82190653581357\n",
      "5999 8.817165451301104\n",
      "8999 8.81716541000788\n",
      "11999 8.817165410007025\n",
      "14999 8.817165410007025\n",
      "17999 8.817165410007027\n",
      "y = -1.7185327486745663e-16 + 0.8567408430737578x + 2.4860231713332116e-17x^2 + -0.09333038904059505x^3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(20000):\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 3000 == 2999:\n",
    "        print(t, loss)\n",
    "    \n",
    "    # 用梯度下降法计算loss对参数的导数\n",
    "    tmp = 2.0 * (y_pred - y)\n",
    "    grad_a = tmp.sum()\n",
    "    grad_b = (tmp * x).sum()\n",
    "    grad_c = (tmp * x**2).sum()\n",
    "    grad_d = (tmp * x**3).sum()\n",
    "    \n",
    "    # 更新参数\n",
    "    a -= grad_a * learning_rate\n",
    "    b -= grad_b * learning_rate\n",
    "    c -= grad_c * learning_rate\n",
    "    d -= grad_d * learning_rate\n",
    "\n",
    "print(\"y = {a} + {b}x + {c}x^2 + {d}x^3\".format(a=a, b=b, c=c, d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Tensor\n",
    "\n",
    "为了让Tensor能够在GPU上计算，需要为其指定正确的device。下面用Tensor来实现上面的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 8.839166641235352\n",
      "5999 8.817167282104492\n",
      "8999 8.817166328430176\n",
      "11999 8.817166328430176\n",
      "14999 8.817166328430176\n",
      "17999 8.817166328430176\n",
      "y = -2.0428718716658523e-09 + 0.8567265868186951x + -1.1004080313625764e-08x^2 + -0.09332836419343948x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")  # 在cpu上运行\n",
    "device = torch.device(\"cuda:0\")  # 在gpu上运行\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(20000):\n",
    "    # forward pass\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    # loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 3000 == 2999:\n",
    "        print(t, loss)\n",
    "    # backprop\n",
    "    tmp = 2.0 * (y_pred - y)\n",
    "\n",
    "    grad_a = tmp.sum()\n",
    "    grad_b = (tmp * x).sum()\n",
    "    grad_c = (tmp * x**2).sum()\n",
    "    grad_d = (tmp * x**3).sum()\n",
    "    \n",
    "    # 更新参数\n",
    "    a -= grad_a * learning_rate\n",
    "    b -= grad_b * learning_rate\n",
    "    c -= grad_c * learning_rate\n",
    "    d -= grad_d * learning_rate\n",
    "    \n",
    "print(\"y = {a} + {b}x + {c}x^2 + {d}x^3\".format(a=a, b=b, c=c, d=d))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Tensor and autograd\n",
    "\n",
    "如果网络非常复杂，自己去计算loss对参数的导数就会变得很麻烦。PyTorch中的autograd包可以帮助我们在反向传播的过程中实现自动求导。使用autograd时，正向传播过程定义了一个计算图，图中 的节点都是Tensor，边则是表示输入与输出之间关系的函数。反向传播时可以轻松计算出梯度信息。对于一个Tensor a，如果将他的requires_grad指定为True，则x.grad是另一个Tensor，存储了a的梯度。下面我们就用自动求导功能实现上述代码，无须再手动执行反向传播的过程了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 tensor(8.8570, grad_fn=<SumBackward0>)\n",
      "5999 tensor(8.8172, grad_fn=<SumBackward0>)\n",
      "8999 tensor(8.8172, grad_fn=<SumBackward0>)\n",
      "11999 tensor(8.8172, grad_fn=<SumBackward0>)\n",
      "14999 tensor(8.8172, grad_fn=<SumBackward0>)\n",
      "17999 tensor(8.8172, grad_fn=<SumBackward0>)\n",
      "y = 3.940505699517871e-09 + 0.8567265868186951x + -1.1153288959064867e-08x^2 + -0.09332836419343948x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")\n",
    "\n",
    "# 默认情况下requires_grad是False，表示在反向传播时我们不需要求loss对它的导数\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, dtype=dtype, device=device)\n",
    "y = torch.sin(x)\n",
    "\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(20000):\n",
    "    \n",
    "    # forward pass\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    \n",
    "    # loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if t % 3000 == 2999:\n",
    "        print(t, loss)\n",
    "    \n",
    "    # 反向传播时通过调用loss.backward()，自动计算loss对所有requires_grad=True的张量的梯度\n",
    "    # 即这里的 a，b，c，d。它们的梯度值保存在 .grad属性中。\n",
    "    loss.backward()\n",
    "    \n",
    "    # 手动更新参数\n",
    "    # 更新参数的操作不需要追踪梯度，因此把它写在torch.no_grad()中，一定不要忘记\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad  # 这里不能写成 a = a- ...，a需要执行in-place运算\n",
    "        b -= learning_rate * b.grad \n",
    "        c -= learning_rate * c.grad \n",
    "        d -= learning_rate * d.grad \n",
    "\n",
    "        # 手动清空梯度缓存，准备下次迭代\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "    \n",
    "print(\"y = {a} + {b}x + {c}x^2 + {d}x^3\".format(a=a, b=b, c=c, d=d))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Defining new autograd functions\n",
    "\n",
    "Torch中所定义的操作f都带有autograd的功能。即只要Tensor的requires_grad设为True，所有涉及该Tensor的操作都能在loss.backward()之后将经过当前操作的梯度向前传递过去。即 grad_in = grad_f(grad_out)，in和out分别是操作f的输入和输出，grad_in是loss对in的梯度，grad_out是loss对out的梯度，grad_f是f的out对in的导数，即链式法则，这是很好理解的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中不可能定义了我们所需要的所有操作，但是我们可以很容易地在PyTorch中自定义带有autograd功能的操作。方法是继承torch.autograd.Function并且实现forward和backward方法。之后在定义计算图时，我们就可以通过实例化并像函数调用一样使用这个自定义的操作了，将Tensor作为输入传入。下面我们自定义个带有autograd功能的三阶勒让德多项式操作：P3(x) = 0.5 * (5x^2 - 3x)。计算图为 y = b * P3(c+dx)。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    我们可以通过继承torch.autograd.Function自己实现一个autograd函数\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        前向传播时接受一个输入Tensor，返回一个输出Tensor。\n",
    "        ctx是一个与上下文相关的对象，存储用于反向传播的各种信息。\n",
    "        使用ctx.save_for_backward可以存储任意用于反向传播的对象。\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * x ** 3 - 3 * x)  # 返回三阶勒让德多项式的结果 \n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        反向传播接受一个Tensor，保存了loss函数对该勒让德多项式输出地方的导数grad_output。\n",
    "        现在我们需要计算loss对该勒让德多项式输入处的导数，让后继续回传给下一个运算环节。\n",
    "        \"\"\"\n",
    "        input, _ = cts.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# # x，y默认的requires_grad为False\n",
    "# x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "# y = torch.sin(x)\n",
    "\n",
    "# a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)  # 返回指定大小，值为fill_value的矩阵 \n",
    "# b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)  # 返回指定大小，值为fill_value的矩阵 \n",
    "# c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)  # 返回指定大小，值为fill_value的矩阵 \n",
    "# d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)  # 返回指定大小，值为fill_value的矩阵 \n",
    "\n",
    "# learning_rate = 5e-6\n",
    "# for t in range(2000):\n",
    "#     # 假设勒让德多项式操作类中包含一个apply方法，返回一个callable的对象(相当于函数)，取别名为P3\n",
    "#     # 此时P3会使用我们自定义的autograd操作\n",
    "#     P3 = LegendrePolynomial3.apply  \n",
    "#     y_pred = a + b + P3(c + d * x)  # c+dx为传入的参数\n",
    "    \n",
    "#     loss = (y_pred - y).pow(2).sum()\n",
    "#     if t % 100 == 99:\n",
    "#         print(t, loss.item())\n",
    "    \n",
    "#     loss.backward()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         a -= learning_rate * a.grad  # 这里不能写成 a = a- ...，a需要执行in-place运算\n",
    "#         b -= learning_rate * b.grad \n",
    "#         c -= learning_rate * c.grad \n",
    "#         d -= learning_rate * d.grad \n",
    "\n",
    "#         # 手动清空梯度缓存，准备下次迭代\n",
    "#         a.grad = None\n",
    "#         b.grad = None\n",
    "#         c.grad = None\n",
    "#         d.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: nn\n",
    "\n",
    "有了autograd，我们可以通过连接各种基础操作定义一些计算图，并实现自动求导。但是对于大规模神经网络，这太底层了。PyTorch中的nn模块实现了对诸多模块更高层的封装，并保存中间所有可训练参数的信息。nn模块也定义了很多使用的loss函数。下面我们就用nn模块实现一个对多项式参数的求取，该多项式为 y = a*x + b*x^2 + c*x^3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "p = torch.tensor([1, 2, 3])  # (3,)\n",
    "xx = x.unsqueeze(-1).pow(p)  # (2000, 3) (x, x^2, x^3) 不同笔的数据永远在第一维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 164.63064575195312\n",
      "199 111.96536254882812\n",
      "299 77.10670471191406\n",
      "399 54.03215026855469\n",
      "499 38.75706481933594\n",
      "599 28.644275665283203\n",
      "699 21.948598861694336\n",
      "799 17.51502799987793\n",
      "899 14.57902717590332\n",
      "999 12.634567260742188\n",
      "1099 11.346598625183105\n",
      "1199 10.493450164794922\n",
      "1299 9.928211212158203\n",
      "1399 9.553703308105469\n",
      "1499 9.305509567260742\n",
      "1599 9.14100456237793\n",
      "1699 9.031965255737305\n",
      "1799 8.959668159484863\n",
      "1899 8.911718368530273\n",
      "1999 8.879920959472656\n",
      "Parameter containing:\n",
      "tensor([-0.0020], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 8.4926e-01,  3.4305e-04, -9.2266e-02]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# nn.Sequential包含一系列的Modules，它接受输入的张量，经过所有包含的Modules后输出新的张量\n",
    "# 因为类中重写了__call__方法，因此返回的实例可以像函数一样被调用\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),  # (2000, 3) x (3, 1) = (2000, 1) 第i行表示 a*xi + b*xi^2 + c*xi^3\n",
    "    torch.nn.Flatten(0, 1)  # 将前两个维度(第0维和第1维)打开\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    y_pred = model(xx)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # backward之前一定不要忘记清空梯度缓存\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # 计算loss对model中所有可训练参数的梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新参数\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# 训练完成，可以像查看list中的元素一样查看第某层的信息\n",
    "linear_layer = model[0]  # 第一层的信息\n",
    "\n",
    "# 查看weight和bias\n",
    "print(linear_layer.bias)  # 只取值的话还需要加上item()\n",
    "print(linear_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前为止我们都是在torch.no_grad中手动更新的参数。因为上面的示例用的是最基本的SGD优化器。对于更复杂优化器，比如Adam，RMSProp等，手动更新参数的过程就太复杂了。torch.optim模块中包含了所有常用的优化器，封装了它们的逻辑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 9156.8232421875\n",
      "199 9149.6064453125\n",
      "299 9143.017578125\n",
      "399 9136.638671875\n",
      "499 9130.294921875\n",
      "599 9123.974609375\n",
      "699 9117.6630859375\n",
      "799 9111.359375\n",
      "899 9105.064453125\n",
      "999 9098.775390625\n",
      "1099 9092.498046875\n",
      "1199 9086.228515625\n",
      "1299 9079.9658203125\n",
      "1399 9073.7119140625\n",
      "1499 9067.466796875\n",
      "1599 9061.228515625\n",
      "1699 9055.0\n",
      "1799 9048.779296875\n",
      "1899 9042.56640625\n",
      "1999 9036.36328125\n",
      "Parameter containing:\n",
      "tensor([0.1389], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0384, -0.4749,  0.0582]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "p = torch.tensor([1, 2, 3])  # (3,)\n",
    "xx = x.unsqueeze(-1).pow(p)  # (2000, 3) (x, x^2, x^3)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),  # (2000, 3) x (3, 1) = (2000, 1) 第i行表示 a*xi + b*xi^2 + c*xi^3\n",
    "    torch.nn.Flatten(0, 1)  # 将前两个维度(第0维和第1维)打开\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# 定义优化器，传入第一个参数是该网络所有需要优化的参数\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(2000):\n",
    "    y_pred = model(xx)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # backward之前一定不要忘记清空梯度缓存\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 计算loss对model中所有可训练参数的梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "\n",
    "# 训练完成，可以像查看list中的元素一样查看第某层的信息\n",
    "linear_layer = model[0]  # 第一层的信息\n",
    "\n",
    "# 查看weight和bias\n",
    "print(linear_layer.bias)  # 只取值的话还需要加上item()\n",
    "print(linear_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Custom nn Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面通过torch.nn.Sequential的方式定义只能定义顺序序列。对于更为复杂的模型，我们需要自定义网络类并继承nn.Module。同时还要定义forward描绘输入与输出之间的逻辑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        在构造函数中定义所有需要优化的参数。\n",
    "        Parameters是Tensor的子类，当用于Module中时有一些特殊的特性：\n",
    "        如果它被指定为Module的成员属性后会被自动添加到model的参数列表中：parameters()，\n",
    "        指定普通的Tensor则没有这种效果，不会缓存它的状态。\n",
    "        If there was no such class as Parameter, these temporaries would get registered too.\n",
    "        \"\"\"\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))  # Parameter的两个参数分别时data，requires_grad=True\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        描绘网络输入与输出之间的关系。可以使用构造函数中定义的模块。\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x**2 + self.d * x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 22678.06640625\n",
      "199 22658.73046875\n",
      "299 22641.072265625\n",
      "399 22623.923828125\n",
      "499 22606.93359375\n",
      "599 22590.013671875\n",
      "699 22573.107421875\n",
      "799 22556.205078125\n",
      "899 22539.31640625\n",
      "999 22522.43359375\n",
      "1099 22505.55859375\n",
      "1199 22488.693359375\n",
      "1299 22471.833984375\n",
      "1399 22454.98828125\n",
      "1499 22438.146484375\n",
      "1599 22421.3125\n",
      "1699 22404.48828125\n",
      "1799 22387.673828125\n",
      "1899 22370.865234375\n",
      "1999 22354.06640625\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "model = Polynomial3()\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(2000):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8115254044532776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor(-0.8115, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(-0.0622, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(0.6445, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(0.2350, requires_grad=True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.a.item())\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Control Flow + Weight Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了说明，下面我们实施一个比较奇怪的模型：一个3-5阶的多项式，y = a + b * x + c * x^2 + d * x^3 + (e * x^exp + ...)，其中exp是一个3-5之间的随机数，并且括号里执行exp次，且每一次都是复用同一个参数e。在定义这个计算图的时候，我们可以直接使用Python中的控制流，比如循环或条件语句。也就是说，如果我们想要在网络中复用某一个部分，完全可以使用Python的语言逻辑来安全、简单地实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.a + self.b * x + self.c * x**2 + self.d * x**3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 1000.8871459960938\n",
      "3999 448.5474548339844\n",
      "5999 196.77584838867188\n",
      "7999 99.22183990478516\n",
      "9999 49.192604064941406\n",
      "11999 26.75980567932129\n",
      "13999 16.95676040649414\n",
      "15999 12.462722778320312\n",
      "17999 10.449708938598633\n",
      "19999 9.54977035522461\n",
      "21999 9.153962135314941\n",
      "23999 8.825005531311035\n",
      "25999 8.905901908874512\n",
      "27999 8.675395965576172\n",
      "29999 8.604968070983887\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "model = DynamicNet()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "for t in range(30000):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0019167063292115927\n",
      "0.8544856309890747\n",
      "-0.00019232860358897597\n",
      "-0.09328288584947586\n",
      "0.00010245972953271121\n"
     ]
    }
   ],
   "source": [
    "print(model.a.item())\n",
    "print(model.b.item())\n",
    "print(model.c.item())\n",
    "print(model.d.item())\n",
    "print(model.e.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.7",
   "language": "python",
   "name": "torch_1.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
